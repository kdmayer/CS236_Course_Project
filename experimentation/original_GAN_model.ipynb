{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import plotly as plt\n",
    "import os\n",
    "import pprint\n",
    "import argparse\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric\n",
    "\n",
    "# Batched CD (CPU), borrowed from https://github.com/ThibaultGROUEIX/AtlasNet\n",
    "def cd_cpu(sample, ref):\n",
    "    x, y = sample, ref\n",
    "    bs, num_points, points_dim = x.size()\n",
    "    xx = torch.bmm(x, x.transpose(2, 1))\n",
    "    yy = torch.bmm(y, y.transpose(2, 1))\n",
    "    zz = torch.bmm(x, y.transpose(2, 1))\n",
    "    diag_ind = torch.arange(0, num_points).to(sample).long()\n",
    "    rx = xx[:, diag_ind, diag_ind].unsqueeze(1).expand_as(xx)\n",
    "    ry = yy[:, diag_ind, diag_ind].unsqueeze(1).expand_as(yy)\n",
    "    P = (rx.transpose(2, 1) + ry - 2 * zz)\n",
    "    return P.min(1)[0], P.min(2)[0]\n",
    "\n",
    "def compute_cd(x, y, reduce_func=torch.mean):\n",
    "    d1, d2 = cd_cpu(x, y)\n",
    "    return reduce_func(d1, dim=1) + reduce_func(d2, dim=1)\n",
    "\n",
    "\n",
    "#def compute_emd(x, y):\n",
    "#    return match_cost(x, y) / x.size(1)\n",
    "\n",
    "\n",
    "def compute_pairwise_cd_emd(x, y, batch_size=32):\n",
    "    NX, NY, cd, _ = x.size(0), y.size(0), [], []\n",
    "    y = y.contiguous()\n",
    "    for i in tqdm(range(NX)):\n",
    "        cdx, _ , xi = [], [], x[i]\n",
    "        for j in range(0, NY, batch_size):\n",
    "            yb = y[j : j + batch_size]\n",
    "            xb = xi.view(1, -1, 3).expand_as(yb).contiguous()\n",
    "            cdx.append(compute_cd(xb, yb).view(1, -1))\n",
    "            #emdx.append(compute_emd(xb, yb).view(1, -1))\n",
    "        cd.append(torch.cat(cdx, dim=1))\n",
    "        #emd.append(torch.cat(emdx, dim=1))\n",
    "    cd = torch.cat(cd, dim=0) #, torch.cat(emd, dim=0)\n",
    "    return cd\n",
    "\n",
    "\n",
    "def compute_mmd_cov(dxy):\n",
    "    _, min_idx = dxy.min(dim=1)\n",
    "    min_val, _ = dxy.min(dim=0)\n",
    "    mmd = min_val.mean()\n",
    "    cov = min_idx.unique().numel() / dxy.size(1)\n",
    "    cov = torch.tensor(cov).to(dxy)\n",
    "    return mmd, cov\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(x, y, batch_size):\n",
    "    cd_yx = compute_pairwise_cd_emd(y, x, batch_size)\n",
    "    mmd_cd, cov_cd = compute_mmd_cov(cd_yx.t())\n",
    "    #mmd_emd, _ = compute_mmd_cov(emd_yx.t())\n",
    "    return {\n",
    "        \"COV-CD\": cov_cd.cpu(),\n",
    "        #\"COV-EMD\": cov_emd.cpu(),\n",
    "        \"MMD-CD\": mmd_cd.cpu(),\n",
    "        #\"MMD-EMD\": mmd_emd.cpu(),\n",
    "    }, {\n",
    "        \"CD_YX\": cd_yx.cpu(),\n",
    "        #\"EMD_YX\": emd_yx.cpu(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "root_dir = \"/Users/kevin/projects/cs236g/default-project\"\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "ckpt_dir = os.path.join(root_dir, \"checkpoints\")\n",
    "# Name of current experiment. Checkpoints will be stored in '{ckpt_dir}/{name}/'. \n",
    "name = \"exp1\"\n",
    "# Manual seed for reproducibility.\n",
    "seed = 0 \n",
    "# point cloud category\n",
    "cate = \"airplane\"\n",
    "# Resumes training using the last checkpoint in ckpt_dir.\n",
    "resume = False\n",
    "batch_size = 64\n",
    "# Number of points sampled from each training sample.\n",
    "tr_sample_size = 10\n",
    "# Number of points sampled from each testing sample.\n",
    "te_sample_size = 10\n",
    "# Total training epoch.\n",
    "max_epoch = 2000\n",
    "# Number of discriminator updates before a generator update.\n",
    "repeat_d = 5\n",
    "log_every_n_step = 20\n",
    "val_every_n_epoch = 20\n",
    "ckpt_every_n_epoch = 100\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "def plot_samples(samples, num=8, rows=2, cols=4):\n",
    "    fig = plt.subplots.make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        specs=[[{\"type\": \"Scatter3d\"} for _ in range(cols)] for _ in range(rows)],\n",
    "    )\n",
    "    indices = torch.randperm(samples.size(0))[:num]\n",
    "    for i, sample in enumerate(samples[indices].cpu()):\n",
    "        fig.add_trace(\n",
    "            plt.graph_objects.Scatter3d(\n",
    "                x=sample[:, 0],\n",
    "                y=sample[:, 2],\n",
    "                z=sample[:, 1],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=3, opacity=0.8),\n",
    "            ),\n",
    "            row=i // cols + 1,\n",
    "            col=i % cols + 1,\n",
    "        )\n",
    "    fig.update_layout(showlegend=False)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.py\n",
    "\n",
    "_synsetid_to_cate = {\n",
    "    \"02691156\": \"airplane\",\n",
    "    \"02958343\": \"car\",\n",
    "    \"03001627\": \"chair\",\n",
    "}\n",
    "_cate_to_synsetid = {v: k for k, v in _synsetid_to_cate.items()}\n",
    "\n",
    "\n",
    "class ShapeNet15k(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, cate, split, random_sample, sample_size):\n",
    "        self.data = []\n",
    "        cate_dir = os.path.join(root, _cate_to_synsetid[cate], split)\n",
    "        for fname in os.listdir(cate_dir):\n",
    "            if fname.endswith(\".npy\"):\n",
    "                path = os.path.join(cate_dir, fname)\n",
    "                sample = np.load(path)[np.newaxis, ...]\n",
    "                self.data.append(torch.from_numpy(sample).float())\n",
    "\n",
    "        # Normalize data\n",
    "        self.data = torch.cat(self.data, dim=0)\n",
    "        self.mu = self.data.view(-1, 3).mean(dim=0).view(1, 3)\n",
    "        self.std = self.data.view(-1).std(dim=0).view(1, 1)\n",
    "        self.data = (self.data - self.mu) / self.std\n",
    "\n",
    "        # Following lines are purely for reproducing results of\n",
    "        # the official SetVAE implementation: github.com/jw9730/setvae\n",
    "        tr_data, te_data = self.data.split(10000, dim=1)\n",
    "        self.data = tr_data if split == \"train\" else te_data\n",
    "\n",
    "        self.random_sample = random_sample\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        sample_idx = (\n",
    "            torch.randperm(x.size(0))[: self.sample_size]\n",
    "            if self.random_sample\n",
    "            else torch.arange(self.sample_size)\n",
    "        )\n",
    "        x = x[sample_idx]\n",
    "        return x, self.mu, self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model.py\n",
    "\n",
    "class MaxBlock(nn.Module):\n",
    "    # Just a linear layer\n",
    "    # Order of the points does not matter. No matter the order of the points\n",
    "    # the output should be the same\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xm, _ = x.max(dim=1, keepdim=True)\n",
    "        x = self.proj(x - xm)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    # You take a point cloud, i.e. (2000, 3) and encode into a latent space, i.e. with e.g. 64 dimensions,\n",
    "    # and then you add some noise to the 64 dimensions and decode it back into a point cloud\n",
    "    # x_dim is the dimension of the point cloud, i.e. 3 (x,y,z)\n",
    "    # d_dim\n",
    "    # z1_dim\n",
    "    \n",
    "    def __init__(self, x_dim, d_dim, z1_dim):\n",
    "        super().__init__()\n",
    "        self.phi = nn.Sequential(\n",
    "            MaxBlock(x_dim, d_dim),\n",
    "            nn.Tanh(),\n",
    "            MaxBlock(d_dim, d_dim),\n",
    "            nn.Tanh(),\n",
    "            MaxBlock(d_dim, d_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.ro = nn.Sequential(\n",
    "            nn.Linear(d_dim, d_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_dim, z1_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.phi(x)\n",
    "        x, _ = x.max(dim=1)\n",
    "        z1 = self.ro(x)\n",
    "        return z1\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_dim, z1_dim, z2_dim, h_dim=512):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(z1_dim, h_dim)\n",
    "        self.fu = nn.Linear(z2_dim, h_dim, bias=False)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, x_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "        x = self.fc(z1) + self.fu(z2)\n",
    "        o = self.dec(x)\n",
    "        return o\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    # The Generator generates one 3D point at a time given conditioned on some random normal noise\n",
    "    def __init__(self, x_dim=3, d_dim=256, z1_dim=256, z2_dim=10):\n",
    "        super().__init__()\n",
    "        self.z2_dim = z2_dim\n",
    "        self.enc = Encoder(x_dim, d_dim, z1_dim)\n",
    "        self.dec = Decoder(x_dim, z1_dim, z2_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        z1 = self.enc(x).unsqueeze(dim=1)\n",
    "        return z1\n",
    "\n",
    "    def decode(self, z1, B, N, device):\n",
    "        # z1 this is a latent vector specifying the class, this is the output of the encoder which takes\n",
    "        # in the point cloud and encodes it into a latent space\n",
    "        # z2 is the random noise used to generate new points individually\n",
    "        z2 = torch.randn((B, N, self.z2_dim)).to(device)\n",
    "        # output is a batch of points, and the points are uniformly distributed on the surface of the object \n",
    "        # that you are trying to model\n",
    "        o = self.dec(z1, z2)\n",
    "        return o\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the point cloud, basically [N, 3], with N being the individual points\n",
    "        z1 = self.encode(x)\n",
    "        # z1 is the latent vector\n",
    "        # o is another point cloud, also [N, 3]\n",
    "        o = self.decode(z1, x.size(0), x.size(1), x.device)\n",
    "        return o, z1\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim=3, z1_dim=256, h_dim=1024, o_dim=1):\n",
    "        # z1_dim: dimension of the latent vector\n",
    "        # o_dim: dimension of the output, which is a scalar that the discriminator aims \n",
    "        # to maximize while the generator aims to minimize\n",
    "         \n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(z1_dim, h_dim)\n",
    "        self.fu = nn.Linear(x_dim, h_dim, bias=False)\n",
    "        self.d1 = nn.Sequential(\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim - z1_dim),\n",
    "        )\n",
    "        self.sc = nn.Linear(z1_dim, h_dim)\n",
    "        self.su = nn.Linear(h_dim - z1_dim, h_dim, bias=False)\n",
    "        self.d2 = nn.Sequential(\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim - z1_dim),\n",
    "        )\n",
    "        self.tc = nn.Linear(z1_dim, h_dim)\n",
    "        self.tu = nn.Linear(h_dim - z1_dim, h_dim, bias=False)\n",
    "        self.d3 = nn.Sequential(\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, o_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z1):\n",
    "        y = self.fc(z1) + self.fu(x)\n",
    "        o = self.d1(y)\n",
    "        y = self.sc(z1) + self.su(o)\n",
    "        o = self.d2(y)\n",
    "        y = self.tc(z1) + self.tu(o)\n",
    "        o = self.d3(y)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer.py\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        net_g,\n",
    "        device,\n",
    "        batch_size,\n",
    "        net_d=None,\n",
    "        opt_g=None,\n",
    "        opt_d=None,\n",
    "        sch_g=None,\n",
    "        sch_d=None,\n",
    "        max_epoch=None,\n",
    "        repeat_d=None,\n",
    "        log_every_n_step=None,\n",
    "        val_every_n_epoch=None,\n",
    "        ckpt_every_n_epoch=None,\n",
    "        ckpt_dir=None,\n",
    "    ):\n",
    "        self.net_g = net_g.to(device)\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.net_d = net_d and net_d.to(device)\n",
    "        self.opt_g = opt_g\n",
    "        self.opt_d = opt_d\n",
    "        self.sch_g = sch_g\n",
    "        self.sch_d = sch_d\n",
    "        self.step = 0\n",
    "        self.epoch = 0\n",
    "        self.max_epoch = max_epoch\n",
    "        self.repeat_d = repeat_d\n",
    "        self.log_every_n_step = log_every_n_step\n",
    "        self.val_every_n_epoch = val_every_n_epoch\n",
    "        self.ckpt_every_n_epoch = ckpt_every_n_epoch\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "\n",
    "    def _state_dict(self):\n",
    "        return {\n",
    "            \"net_g\": self.net_g.state_dict(),\n",
    "            \"net_d\": self.net_d.state_dict(),\n",
    "            \"opt_g\": self.opt_g.state_dict(),\n",
    "            \"opt_d\": self.opt_d.state_dict(),\n",
    "            \"sch_g\": self.sch_g.state_dict(),\n",
    "            \"sch_d\": self.sch_d.state_dict(),\n",
    "            \"step\": self.step,\n",
    "            \"epoch\": self.epoch,\n",
    "            \"max_epoch\": self.max_epoch,\n",
    "        }\n",
    "\n",
    "    def _load_state_dict(self, state_dict):\n",
    "        for k, m in {\n",
    "            \"net_g\": self.net_g,\n",
    "            \"net_d\": self.net_d,\n",
    "            \"opt_g\": self.opt_g,\n",
    "            \"opt_d\": self.opt_d,\n",
    "            \"sch_g\": self.sch_g,\n",
    "            \"sch_d\": self.sch_d,\n",
    "        }.items():\n",
    "            m and m.load_state_dict(state_dict[k])\n",
    "        self.step, self.epoch, self.max_epoch = map(\n",
    "            state_dict.get,\n",
    "            (\n",
    "                \"step\",\n",
    "                \"epoch\",\n",
    "                \"max_epoch\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        ckpt_path = os.path.join(self.ckpt_dir, f\"{self.epoch}.pth\")\n",
    "        torch.save(self._state_dict(), ckpt_path)\n",
    "\n",
    "    def load_checkpoint(self, ckpt_path=None):\n",
    "        if not ckpt_path:  # Find last checkpoint in ckpt_dir\n",
    "            ckpt_paths = [p for p in os.listdir(self.ckpt_dir) if p.endswith(\".pth\")]\n",
    "            assert ckpt_paths, \"No checkpoints found.\"\n",
    "            ckpt_path = sorted(ckpt_paths, key=lambda f: int(f[:-4]))[-1]\n",
    "            ckpt_path = os.path.join(self.ckpt_dir, ckpt_path)\n",
    "        self._load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "    def _train_step_g(self, x, mu, std):\n",
    "        o, z1 = self.net_g(x)\n",
    "        op = self.net_d(o, z1.detach())\n",
    "        # This is the GAN loss\n",
    "        loss_op = -op.mean()\n",
    "        # TODO: This is the point cloud loss. You can modify the chamfer loss here\n",
    "        # o is model output, prediction\n",
    "        # x is ground truth\n",
    "        loss_cd = compute_cd(o, x, reduce_func=torch.sum).mean()\n",
    "        return loss_op + loss_cd\n",
    "\n",
    "    def _train_step_d(self, x, mu, std):\n",
    "        o, z1 = self.net_g(x)\n",
    "        xp = self.net_d(x, z1.detach())\n",
    "        op = self.net_d(o.detach(), z1.detach())\n",
    "        loss_d = F.relu(1.0 - xp).mean() + F.relu(1.0 + op).mean()\n",
    "        return loss_d\n",
    "\n",
    "    def train(self, train_loader, val_loader):\n",
    "        while self.epoch < self.max_epoch:\n",
    "\n",
    "            # Validation and checkpointing\n",
    "            if self.epoch % self.val_every_n_epoch == 0:\n",
    "                (metrics, _), samples = self.test(val_loader)\n",
    "                wandb.log({**metrics, \"samples\": samples, \"epoch\": self.epoch})\n",
    "            if self.epoch % self.ckpt_every_n_epoch == 0:\n",
    "                self.save_checkpoint()\n",
    "\n",
    "            with tqdm(train_loader) as t:\n",
    "                self.net_g.train()\n",
    "                self.net_d.train()\n",
    "                for batch in t:\n",
    "                    batch = [t.to(self.device) for t in batch]\n",
    "\n",
    "                    # Update step\n",
    "                    loss_d = self._train_step_d(*batch)\n",
    "                    self.opt_d.zero_grad()\n",
    "                    loss_d.backward()\n",
    "                    self.opt_d.step()\n",
    "                    if self.step % self.repeat_d == 0:\n",
    "                        loss_g = self._train_step_g(*batch)\n",
    "                        self.opt_g.zero_grad()\n",
    "                        loss_g.backward()\n",
    "                        self.opt_g.step()\n",
    "\n",
    "                    # Stepwise logging\n",
    "                    t.set_description(\n",
    "                        f\"Epoch:{self.epoch}|L(G):{loss_g.item():.2f}|L(D):{loss_d.item():.2f}\"\n",
    "                    )\n",
    "                    if self.step % self.log_every_n_step == 0:\n",
    "                        wandb.log(\n",
    "                            {\n",
    "                                \"loss_g\": loss_g.cpu(),\n",
    "                                \"loss_d\": loss_d.cpu(),\n",
    "                                \"step\": self.step,\n",
    "                                \"epoch\": self.epoch,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    self.step += 1\n",
    "                self.sch_g.step()\n",
    "                self.sch_d.step()\n",
    "            self.epoch += 1\n",
    "\n",
    "    def _test_step(self, x, mu, std):\n",
    "        o, _ = self.net_g(x)\n",
    "        x, o = x * std + mu, o * std + mu  # denormalize\n",
    "        return o, x\n",
    "\n",
    "    def _test_end(self, o, x):\n",
    "        metrics = compute_metrics(o, x, self.batch_size)\n",
    "        samples = plot_samples(o)\n",
    "        return metrics, samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, test_loader):\n",
    "        results = []\n",
    "        self.net_g.eval()\n",
    "        for batch in tqdm(test_loader):\n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "            results.append(self._test_step(*batch))\n",
    "        return self._test_end(*(torch.cat(_, dim=0) for _ in zip(*results)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:tr10uu6g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6155... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">honest-river-11</strong>: <a href=\"https://wandb.ai/kmyr/pcgan/runs/tr10uu6g\" target=\"_blank\">https://wandb.ai/kmyr/pcgan/runs/tr10uu6g</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220223_191716-tr10uu6g/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:tr10uu6g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kmyr/pcgan/runs/65f2hgo6\" target=\"_blank\">lemon-wind-12</a></strong> to <a href=\"https://wandb.ai/kmyr/pcgan\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 40.57it/s]\n",
      "100%|██████████| 405/405 [00:01<00:00, 326.69it/s]\n",
      "Epoch:0|L(G):34.31|L(D):2.00: 100%|██████████| 44/44 [00:29<00:00,  1.49it/s]\n",
      "Epoch:1|L(G):34.27|L(D):2.00:  11%|█▏        | 5/44 [00:04<00:32,  1.21it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-ad028173dbae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-8fe7167cd67e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0mloss_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_step_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                     \u001b[0mloss_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_d\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cs236/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cs236/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fix seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Setup checkpoint directory\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.mkdir(ckpt_dir)\n",
    "ckpt_subdir = os.path.join(ckpt_dir, name)\n",
    "if not os.path.exists(ckpt_subdir):\n",
    "    os.mkdir(ckpt_subdir)\n",
    "\n",
    "# Setup logging\n",
    "wandb.init(project=\"pcgan\")\n",
    "\n",
    "# Setup dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=ShapeNet15k(\n",
    "        root=data_dir,\n",
    "        cate=cate,\n",
    "        split=\"train\",\n",
    "        random_sample=True,\n",
    "        sample_size=tr_sample_size,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=ShapeNet15k(\n",
    "        root=data_dir,\n",
    "        cate=cate,\n",
    "        split=\"val\",\n",
    "        random_sample=False,\n",
    "        sample_size=te_sample_size,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# Setup model, optimizer and scheduler\n",
    "net_g = Generator()\n",
    "net_d = Discriminator()\n",
    "opt_g = torch.optim.Adam(net_g.parameters(), lr=4e-4, betas=(0.9, 0.999))\n",
    "opt_d = torch.optim.Adam(net_d.parameters(), lr=2e-4, betas=(0.9, 0.999))\n",
    "sch_g = torch.optim.lr_scheduler.LambdaLR(opt_g, lr_lambda=lambda e: 1.0)\n",
    "sch_d = torch.optim.lr_scheduler.LambdaLR(opt_d, lr_lambda=lambda e: 1.0)\n",
    "\n",
    "# Setup trainer\n",
    "trainer = Trainer(\n",
    "    net_g=net_g,\n",
    "    net_d=net_d,\n",
    "    opt_g=opt_g,\n",
    "    opt_d=opt_d,\n",
    "    sch_g=sch_g,\n",
    "    sch_d=sch_d,\n",
    "    device=device,\n",
    "    batch_size=batch_size,\n",
    "    max_epoch=max_epoch,\n",
    "    repeat_d=repeat_d,\n",
    "    log_every_n_step=log_every_n_step,\n",
    "    val_every_n_epoch=val_every_n_epoch,\n",
    "    ckpt_every_n_epoch=ckpt_every_n_epoch,\n",
    "    ckpt_dir=ckpt_subdir,\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "if resume:\n",
    "    trainer.load_checkpoint()\n",
    "\n",
    "# Start training\n",
    "trainer.train(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
