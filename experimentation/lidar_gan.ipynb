{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a664018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import plotly as plt\n",
    "import os\n",
    "import pprint\n",
    "import argparse\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d72056d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4725)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([0.4304, 0.5146])\n",
    "x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df21e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric\n",
    "\n",
    "# Batched CD (CPU), borrowed from https://github.com/ThibaultGROUEIX/AtlasNet\n",
    "def cd_cpu(sample, ref):\n",
    "    x, y = sample, ref\n",
    "    bs, num_points, points_dim = x.size()\n",
    "    xx = torch.bmm(x, x.transpose(2, 1))\n",
    "    yy = torch.bmm(y, y.transpose(2, 1))\n",
    "    zz = torch.bmm(x, y.transpose(2, 1))\n",
    "    diag_ind = torch.arange(0, num_points).to(sample).long()\n",
    "    rx = xx[:, diag_ind, diag_ind].unsqueeze(1).expand_as(xx)\n",
    "    ry = yy[:, diag_ind, diag_ind].unsqueeze(1).expand_as(yy)\n",
    "    P = (rx.transpose(2, 1) + ry - 2 * zz)\n",
    "    return P.min(1)[0], P.min(2)[0]\n",
    "\n",
    "def compute_cd(x, y, reduce_func=torch.mean):\n",
    "    d1, d2 = cd_cpu(x, y)\n",
    "    return reduce_func(d1, dim=1) + reduce_func(d2, dim=1)\n",
    "\n",
    "\n",
    "#def compute_emd(x, y):\n",
    "#    return match_cost(x, y) / x.size(1)\n",
    "\n",
    "\n",
    "def compute_pairwise_cd_emd(x, y, batch_size=32):\n",
    "    NX, NY, cd, _ = x.size(0), y.size(0), [], []\n",
    "    y = y.contiguous()\n",
    "    for i in tqdm(range(NX)):\n",
    "        cdx, _ , xi = [], [], x[i]\n",
    "        for j in range(0, NY, batch_size):\n",
    "            yb = y[j : j + batch_size]\n",
    "            xb = xi.view(1, -1, 3).expand_as(yb).contiguous()\n",
    "            cdx.append(compute_cd(xb, yb).view(1, -1))\n",
    "            #emdx.append(compute_emd(xb, yb).view(1, -1))\n",
    "        cd.append(torch.cat(cdx, dim=1))\n",
    "        #emd.append(torch.cat(emdx, dim=1))\n",
    "    cd = torch.cat(cd, dim=0) #, torch.cat(emd, dim=0)\n",
    "    return cd\n",
    "\n",
    "\n",
    "def compute_mmd_cov(dxy):\n",
    "    _, min_idx = dxy.min(dim=1)\n",
    "    min_val, _ = dxy.min(dim=0)\n",
    "    mmd = min_val.mean()\n",
    "    cov = min_idx.unique().numel() / dxy.size(1)\n",
    "    cov = torch.tensor(cov).to(dxy)\n",
    "    return mmd, cov\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(x, y, batch_size):\n",
    "    cd_yx = compute_pairwise_cd_emd(y, x, batch_size)\n",
    "    mmd_cd, cov_cd = compute_mmd_cov(cd_yx.t())\n",
    "    #mmd_emd, _ = compute_mmd_cov(emd_yx.t())\n",
    "    return {\n",
    "        \"COV-CD\": cov_cd.cpu(),\n",
    "        #\"COV-EMD\": cov_emd.cpu(),\n",
    "        \"MMD-CD\": mmd_cd.cpu(),\n",
    "        #\"MMD-EMD\": mmd_emd.cpu(),\n",
    "    }, {\n",
    "        \"CD_YX\": cd_yx.cpu(),\n",
    "        #\"EMD_YX\": emd_yx.cpu(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00658e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "root_dir = \"/Users/kevin/projects/cs236g/default-project\"\n",
    "data_dir = os.path.join(\"/Users/kevin/CS236G\", \"data\")\n",
    "ckpt_dir = os.path.join(root_dir, \"checkpoints\")\n",
    "# Name of current experiment. Checkpoints will be stored in '{ckpt_dir}/{name}/'. \n",
    "name = \"exp1\"\n",
    "# Manual seed for reproducibility.\n",
    "seed = 0 \n",
    "# point cloud category\n",
    "cate = \"airplane\"\n",
    "# Resumes training using the last checkpoint in ckpt_dir.\n",
    "resume = False\n",
    "batch_size = 8\n",
    "# Number of points sampled from each training sample.\n",
    "tr_sample_size = 10\n",
    "# Number of points sampled from each testing sample.\n",
    "te_sample_size = 10\n",
    "# Total training epoch.\n",
    "max_epoch = 2000\n",
    "# Number of discriminator updates before a generator update.\n",
    "repeat_d = 5\n",
    "log_every_n_step = 20\n",
    "val_every_n_epoch = 20\n",
    "ckpt_every_n_epoch = 100\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "def plot_samples(samples, num=8, rows=2, cols=4):\n",
    "    fig = plt.subplots.make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        specs=[[{\"type\": \"Scatter3d\"} for _ in range(cols)] for _ in range(rows)],\n",
    "    )\n",
    "    indices = torch.randperm(samples.size(0))[:num]\n",
    "    for i, sample in enumerate(samples[indices].cpu()):\n",
    "        fig.add_trace(\n",
    "            plt.graph_objects.Scatter3d(\n",
    "                x=sample[:, 0],\n",
    "                y=sample[:, 2],\n",
    "                z=sample[:, 1],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=3, opacity=0.8),\n",
    "            ),\n",
    "            row=i // cols + 1,\n",
    "            col=i % cols + 1,\n",
    "        )\n",
    "    fig.update_layout(showlegend=False)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c71928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.py\n",
    "\n",
    "# split is either \"Train\", \"Val\", \"Test\"\n",
    "class Lidar(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, split): #random_sample, sample_size\n",
    "        self.data = []\n",
    "        for fname in os.listdir(os.path.join(data_dir, split)):\n",
    "            if fname.endswith(\".npy\"):\n",
    "                path = os.path.join(data_dir, split, fname)\n",
    "                # you add an extra dimension\n",
    "                # same as torch.unsqueeze but for numpy\n",
    "                sample = np.load(path)[np.newaxis, ...]\n",
    "                self.data.append(torch.from_numpy(sample).float())\n",
    "\n",
    "        # Normalize data\n",
    "        # concat observations along first dim\n",
    "        self.data = torch.cat(self.data, dim=0)\n",
    "        \n",
    "        # Comment out because our data is already min-max-scaled\n",
    "        #self.mu = self.data.view(-1, 3).mean(dim=0).view(1, 3)\n",
    "        #self.std = self.data.view(-1).std(dim=0).view(1, 1)\n",
    "        #self.data = (self.data - self.mu) / self.std\n",
    "\n",
    "        # Following lines are purely for reproducing results of\n",
    "        # the official SetVAE implementation: github.com/jw9730/setvae\n",
    "        #tr_data, te_data = self.data.split(10000, dim=1)\n",
    "        #self.data = tr_data if split == \"train\" else te_data\n",
    "\n",
    "        #self.random_sample = random_sample\n",
    "        #self.sample_size = sample_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        \"\"\"\n",
    "        sample_idx = (\n",
    "            torch.randperm(x.size(0))[: self.sample_size]\n",
    "            if self.random_sample\n",
    "            else torch.arange(self.sample_size)\n",
    "        )\n",
    "        x = x[sample_idx]\n",
    "        \"\"\"\n",
    "        return x #,self.mu, self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbac20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model.py\n",
    "\n",
    "class MaxBlock(nn.Module):\n",
    "    # Just a linear layer\n",
    "    # Order of the points does not matter. No matter the order of the points\n",
    "    # the output should be the same\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xm, _ = x.max(dim=1, keepdim=True)\n",
    "        x = self.proj(x - xm)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    # You take a point cloud, i.e. (2000, 3) and encode into a latent space, i.e. with e.g. 64 dimensions,\n",
    "    # and then you add some noise to the 64 dimensions and decode it back into a point cloud\n",
    "    # x_dim is the dimension of the point cloud, i.e. 3 (x,y,z)\n",
    "    # d_dim\n",
    "    # z1_dim\n",
    "    \n",
    "    def __init__(self, x_dim, d_dim, z1_dim):\n",
    "        super().__init__()\n",
    "        self.phi = nn.Sequential(\n",
    "            MaxBlock(x_dim, d_dim),\n",
    "            nn.Tanh(),\n",
    "            MaxBlock(d_dim, d_dim),\n",
    "            nn.Tanh(),\n",
    "            MaxBlock(d_dim, d_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.ro = nn.Sequential(\n",
    "            nn.Linear(d_dim, d_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_dim, z1_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.phi(x)\n",
    "        x, _ = x.max(dim=1)\n",
    "        z1 = self.ro(x)\n",
    "        return z1\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_dim, z1_dim, z2_dim, h_dim=512):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(z1_dim, h_dim)\n",
    "        self.fu = nn.Linear(z2_dim, h_dim, bias=False)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, x_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "        x = self.fc(z1) + self.fu(z2)\n",
    "        o = self.dec(x)\n",
    "        return o\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    # The Generator generates one 3D point at a time given conditioned on some random normal noise\n",
    "    def __init__(self, x_dim=3, d_dim=256, z1_dim=256, z2_dim=10):\n",
    "        super().__init__()\n",
    "        self.z2_dim = z2_dim\n",
    "        self.enc = Encoder(x_dim, d_dim, z1_dim)\n",
    "        self.dec = Decoder(x_dim, z1_dim, z2_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        z1 = self.enc(x).unsqueeze(dim=1)\n",
    "        return z1\n",
    "\n",
    "    def decode(self, z1, B, N, device):\n",
    "        # z1 this is a latent vector specifying the class, this is the output of the encoder which takes\n",
    "        # in the point cloud and encodes it into a latent space\n",
    "        # z2 is the random noise used to generate new points individually\n",
    "        z2 = torch.randn((B, N, self.z2_dim)).to(device)\n",
    "        # output is a batch of points, and the points are uniformly distributed on the surface of the object \n",
    "        # that you are trying to model\n",
    "        o = self.dec(z1, z2)\n",
    "        return o\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the point cloud, basically [N, 3], with N being the individual points\n",
    "        z1 = self.encode(x)\n",
    "        # z1 is the latent vector\n",
    "        # o is another point cloud, also [N, 3]\n",
    "        o = self.decode(z1, x.size(0), x.size(1), x.device)\n",
    "        return o, z1\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim=3, z1_dim=256, h_dim=1024, o_dim=1):\n",
    "        # z1_dim: dimension of the latent vector\n",
    "        # o_dim: dimension of the output, which is a scalar that the discriminator aims \n",
    "        # to maximize while the generator aims to minimize\n",
    "         \n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(z1_dim, h_dim)\n",
    "        self.fu = nn.Linear(x_dim, h_dim, bias=False)\n",
    "        self.d1 = nn.Sequential(\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim - z1_dim),\n",
    "        )\n",
    "        self.sc = nn.Linear(z1_dim, h_dim)\n",
    "        self.su = nn.Linear(h_dim - z1_dim, h_dim, bias=False)\n",
    "        self.d2 = nn.Sequential(\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim - z1_dim),\n",
    "        )\n",
    "        self.tc = nn.Linear(z1_dim, h_dim)\n",
    "        self.tu = nn.Linear(h_dim - z1_dim, h_dim, bias=False)\n",
    "        self.d3 = nn.Sequential(\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(h_dim, o_dim),\n",
    "            # You can add a Softmax here and adjust the o_dim to be the class\n",
    "            # This one is currently conditioned on z1\n",
    "            # google point cloud classification.\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z1):\n",
    "        y = self.fc(z1) + self.fu(x)\n",
    "        o = self.d1(y)\n",
    "        y = self.sc(z1) + self.su(o)\n",
    "        o = self.d2(y)\n",
    "        y = self.tc(z1) + self.tu(o)\n",
    "        o = self.d3(y)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer.py\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        net_g,\n",
    "        device,\n",
    "        batch_size,\n",
    "        net_d=None,\n",
    "        opt_g=None,\n",
    "        opt_d=None,\n",
    "        sch_g=None,\n",
    "        sch_d=None,\n",
    "        max_epoch=None,\n",
    "        repeat_d=None,\n",
    "        log_every_n_step=None,\n",
    "        val_every_n_epoch=None,\n",
    "        ckpt_every_n_epoch=None,\n",
    "        ckpt_dir=None,\n",
    "    ):\n",
    "        self.net_g = net_g.to(device)\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.net_d = net_d and net_d.to(device)\n",
    "        self.opt_g = opt_g\n",
    "        self.opt_d = opt_d\n",
    "        self.sch_g = sch_g\n",
    "        self.sch_d = sch_d\n",
    "        self.step = 0\n",
    "        self.epoch = 0\n",
    "        self.max_epoch = max_epoch\n",
    "        self.repeat_d = repeat_d\n",
    "        self.log_every_n_step = log_every_n_step\n",
    "        self.val_every_n_epoch = val_every_n_epoch\n",
    "        self.ckpt_every_n_epoch = ckpt_every_n_epoch\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "\n",
    "    def _state_dict(self):\n",
    "        return {\n",
    "            \"net_g\": self.net_g.state_dict(),\n",
    "            \"net_d\": self.net_d.state_dict(),\n",
    "            \"opt_g\": self.opt_g.state_dict(),\n",
    "            \"opt_d\": self.opt_d.state_dict(),\n",
    "            \"sch_g\": self.sch_g.state_dict(),\n",
    "            \"sch_d\": self.sch_d.state_dict(),\n",
    "            \"step\": self.step,\n",
    "            \"epoch\": self.epoch,\n",
    "            \"max_epoch\": self.max_epoch,\n",
    "        }\n",
    "\n",
    "    def _load_state_dict(self, state_dict):\n",
    "        for k, m in {\n",
    "            \"net_g\": self.net_g,\n",
    "            \"net_d\": self.net_d,\n",
    "            \"opt_g\": self.opt_g,\n",
    "            \"opt_d\": self.opt_d,\n",
    "            \"sch_g\": self.sch_g,\n",
    "            \"sch_d\": self.sch_d,\n",
    "        }.items():\n",
    "            m and m.load_state_dict(state_dict[k])\n",
    "        self.step, self.epoch, self.max_epoch = map(\n",
    "            state_dict.get,\n",
    "            (\n",
    "                \"step\",\n",
    "                \"epoch\",\n",
    "                \"max_epoch\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        ckpt_path = os.path.join(self.ckpt_dir, f\"{self.epoch}.pth\")\n",
    "        torch.save(self._state_dict(), ckpt_path)\n",
    "\n",
    "    def load_checkpoint(self, ckpt_path=None):\n",
    "        if not ckpt_path:  # Find last checkpoint in ckpt_dir\n",
    "            ckpt_paths = [p for p in os.listdir(self.ckpt_dir) if p.endswith(\".pth\")]\n",
    "            assert ckpt_paths, \"No checkpoints found.\"\n",
    "            ckpt_path = sorted(ckpt_paths, key=lambda f: int(f[:-4]))[-1]\n",
    "            ckpt_path = os.path.join(self.ckpt_dir, ckpt_path)\n",
    "        self._load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "    def _train_step_g(self, x): #, mu, std\n",
    "        o, z1 = self.net_g(x)\n",
    "        op = self.net_d(o, z1.detach())\n",
    "        # This is the GAN loss\n",
    "        loss_op = -op.mean()\n",
    "        # TODO: This is the point cloud loss. You can modify the chamfer loss here\n",
    "        # o is model output, prediction, [B x Number of Points x 3]\n",
    "        # x is ground truth, [B x Number of Points x 3]\n",
    "        # TODO: HERE YOU CAN MODIFY THE LOSS\n",
    "        loss_cd = compute_cd(o, x, reduce_func=torch.sum).mean()\n",
    "        return loss_op + loss_cd\n",
    "\n",
    "    def _train_step_d(self, x): #, mu, std\n",
    "        o, z1 = self.net_g(x)\n",
    "        xp = self.net_d(x, z1.detach())\n",
    "        op = self.net_d(o.detach(), z1.detach())\n",
    "        loss_d = F.relu(1.0 - xp).mean() + F.relu(1.0 + op).mean()\n",
    "        return loss_d\n",
    "\n",
    "    def train(self, train_loader, val_loader):\n",
    "        while self.epoch < self.max_epoch:\n",
    "\n",
    "            # Validation and checkpointing\n",
    "            if self.epoch % self.val_every_n_epoch == 0:\n",
    "                (metrics, _), samples = self.test(val_loader)\n",
    "                wandb.log({**metrics, \"samples\": samples, \"epoch\": self.epoch})\n",
    "            if self.epoch % self.ckpt_every_n_epoch == 0:\n",
    "                self.save_checkpoint()\n",
    "\n",
    "            with tqdm(train_loader) as t:\n",
    "                self.net_g.train()\n",
    "                self.net_d.train()\n",
    "                for batch in t:\n",
    "\n",
    "                    # Update step\n",
    "                    loss_d = self._train_step_d(batch.to(self.device))\n",
    "                    self.opt_d.zero_grad()\n",
    "                    loss_d.backward()\n",
    "                    self.opt_d.step()\n",
    "                    if self.step % self.repeat_d == 0:\n",
    "                        loss_g = self._train_step_g(batch.to(self.device))\n",
    "                        self.opt_g.zero_grad()\n",
    "                        loss_g.backward()\n",
    "                        self.opt_g.step()\n",
    "\n",
    "                    # Stepwise logging\n",
    "                    t.set_description(\n",
    "                        f\"Epoch:{self.epoch}|L(G):{loss_g.item():.2f}|L(D):{loss_d.item():.2f}\"\n",
    "                    )\n",
    "                    if self.step % self.log_every_n_step == 0:\n",
    "                        wandb.log(\n",
    "                            {\n",
    "                                \"loss_g\": loss_g.cpu(),\n",
    "                                \"loss_d\": loss_d.cpu(),\n",
    "                                \"step\": self.step,\n",
    "                                \"epoch\": self.epoch,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    self.step += 1\n",
    "                self.sch_g.step()\n",
    "                self.sch_d.step()\n",
    "            self.epoch += 1\n",
    "\n",
    "    def _test_step(self, x): #, mu, std\n",
    "        o, _ = self.net_g(x)\n",
    "        #x, o = x * std + mu, o * std + mu  # denormalize\n",
    "        return o, x\n",
    "\n",
    "    def _test_end(self, o, x):\n",
    "        # TODO: This is the point cloud loss. You can modify the chamfer loss here\n",
    "        # o is model output, prediction, [B x Number of Points x 3]\n",
    "        # x is ground truth, [B x Number of Points x 3]\n",
    "        # TODO: HERE YOU CAN MODIFY THE LOSS\n",
    "        metrics = compute_metrics(o, x, self.batch_size)\n",
    "        samples = plot_samples(o)\n",
    "        return metrics, samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, test_loader):\n",
    "        results = []\n",
    "        self.net_g.eval()\n",
    "        for batch in tqdm(test_loader):\n",
    "            #batch = [t.to(self.device) for t in batch]\n",
    "            results.append(self._test_step(batch.to(self.device)))\n",
    "        return self._test_end(*(torch.cat(_, dim=0) for _ in zip(*results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Setup checkpoint directory\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.mkdir(ckpt_dir)\n",
    "ckpt_subdir = os.path.join(ckpt_dir, name)\n",
    "if not os.path.exists(ckpt_subdir):\n",
    "    os.mkdir(ckpt_subdir)\n",
    "\n",
    "# Setup logging\n",
    "wandb.init(project=\"pcgan\")\n",
    "\n",
    "# Setup dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=Lidar(\n",
    "        data_dir=data_dir,\n",
    "        split=\"Train\",\n",
    "        #random_sample=True,\n",
    "        #sample_size=tr_sample_size,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=Lidar(\n",
    "        data_dir=data_dir,\n",
    "        split=\"Val\",\n",
    "        #random_sample=False,\n",
    "        #sample_size=te_sample_size,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# Setup model, optimizer and scheduler\n",
    "net_g = Generator()\n",
    "net_d = Discriminator()\n",
    "opt_g = torch.optim.Adam(net_g.parameters(), lr=4e-4, betas=(0.9, 0.999))\n",
    "opt_d = torch.optim.Adam(net_d.parameters(), lr=2e-4, betas=(0.9, 0.999))\n",
    "sch_g = torch.optim.lr_scheduler.LambdaLR(opt_g, lr_lambda=lambda e: 1.0)\n",
    "sch_d = torch.optim.lr_scheduler.LambdaLR(opt_d, lr_lambda=lambda e: 1.0)\n",
    "\n",
    "# Setup trainer\n",
    "trainer = Trainer(\n",
    "    net_g=net_g,\n",
    "    net_d=net_d,\n",
    "    opt_g=opt_g,\n",
    "    opt_d=opt_d,\n",
    "    sch_g=sch_g,\n",
    "    sch_d=sch_d,\n",
    "    device=device,\n",
    "    batch_size=batch_size,\n",
    "    max_epoch=max_epoch,\n",
    "    repeat_d=repeat_d,\n",
    "    log_every_n_step=log_every_n_step,\n",
    "    val_every_n_epoch=val_every_n_epoch,\n",
    "    ckpt_every_n_epoch=ckpt_every_n_epoch,\n",
    "    ckpt_dir=ckpt_subdir,\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "if resume:\n",
    "    trainer.load_checkpoint()\n",
    "\n",
    "# Start training\n",
    "trainer.train(train_loader, val_loader)\n",
    "# Loss of Generator\n",
    "# Loss of Discriminator\n",
    "# Train Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26cb911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b999882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c6fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
